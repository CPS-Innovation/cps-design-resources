

## What is mindful friction?

Mindful friction creates moments for people to pause, think, stay involved and take control when working with AI. Itâ€™s a way to reduce risk to the user, the organisation, and the public, and ensure desired outcomes (such as tool adoption, improved case quality, confidence, and trust).

## Why mindful friction?

Like many other organisations, CPS wants to embrace the transformative possibilities of AI. But we have a mandate to do so responsibly. This means minimising harm to the public, the individuals within the justice system, and to the CPS. Our AI Ethics Framework (link here soon) requires a human oversight process to be part of any services that use AI:

- AI should support humans, not replace them. Ensure adequate human participation in AI processes and verify the accuracy of outputs, by an appropriately qualified and trained person. There should be mechanisms in place for end-users to provide feedback on system performance and accuracy. 

- Remember that AI lacks flexibility, human understanding, and compassion. While humans can take individual circumstances into account on a discretionary basis, AI systems do not have this capacity. <strong>Note: AI should not be used for fully automated decision-making.</strong>

One way of making sure this happens is through mindful friction. Designing a service with friction in mind is a way to reduce risk in human-AI interactions - such as errors leading to unjust outcomes, or skills fading over time, or loss of empathy. Friction is familiar in [many](https://gb.e-guide.renault.com/eng/Austral/Driver-vigilance-warning) other [industries](https://en.wikipedia.org/wiki/Interlocking) to enforce [safe](https://en.wikipedia.org/wiki/Child-resistant_packaging#:~:text=Although%20child%2Dresistant%20lids%20are,standards%20on%20requirements%20and%20protocols.) behaviour. We can use similar ideas to help the people using our software.

## How to use this framework

If your team has identified an AI use case and youâ€™re in the [Discovery phase](https://www.gov.uk/service-manual/agile-delivery/how-the-discovery-phase-works), this framework will help you to design the UX/UI layer of the tool using mindful friction. The goal is to help you <strong>spot potential risks</strong> in human-AI interactions, <strong>identify design solutions</strong> quickly, <strong>make choices efficiently and consistently</strong>, and <strong>embed responsible AI</strong> practises into your work.

This framework is made of 4 steps.

<strong>Step 1:</strong> Identify and prioritise potential risks and harms
<strong>Step 2:</strong> Decide the level of risk
<strong>Step 3:</strong> Identify key moments
<strong>Step 4:</strong> Check for existing solutions


### Also consider

If your team is ready to use this framework, that means you should already have a validated benefits case, understanding of user expectations, needs and current behaviours, and a goal that relates to CPSâ€™ strategic intent.

In parallel, the team should also be conducting technical discovery.


## Step 1: Identify and prioritise potential risks and harms

To use this framework you need to have identified and prioritised the potential risks/harms of your AI use case.
Use the following inputs to find them:

- [Risk taxonomy for human oversight ](https://cpsgovuk-my.sharepoint.com/:x:/g/personal/david_morgan1_cps_gov_uk/EQO3CHMMAPlOv57sre3ZhYwB7EzV9ssk2IKsuirWfIvJFg?e=Cwn3C3)
- [Harms Workshop guidance (Facilitators)](https://cpsgovuk.sharepoint.com/:w:/s/AIDataSecurityandEthics/EVX8vXhgIXRAl917zavsf90BPLmd0c6-2KdZmUIxeCI7Qw?e=bZHqwn&CID=7826C9CB-1EE5-4B12-968F-64EAAA6DEA4E&wdLOR=cE38AC48D-FBFB-4EE6-940E-977BAF04FAD6)
- [Harms Workshop guidance (Participants) ](https://cpsgovuk.sharepoint.com/:w:/s/AIDataSecurityandEthics/EU3XPGIIOzlCozaV3IMJKIYBVy0SOYPvKXevvWIdvf69Mw?e=DzvKRq&CID=2331E765-33E8-4CF3-973D-9477E0425E50&wdLOR=cDF0D67AC-550F-419A-825A-70BBF8830D7B)


### Also consider

It can be helpful to turn your design problems (i.e. â€˜thereâ€™s a risk of creating unjust outcomes for victimsâ€™) into framing questions, for example â€˜How might we...â€™ statements.

To do this, take 1 of your prioritised risks and list out questions for how to protect against that risk. From there, you can start to hypothesise potential solutions.

- <strong>Example:</strong> Risk of unjust outcomes for victims, defendants, witnesses. 

	- How might we prevent users from accepting incorrect AI suggestions, especially when under time pressure?

##  Step 2: Decide the level of risk
1.	Consider your AI use case
2.	Decide what level of risk to assign to it based on the kinds of risks/harms you identified in the previous step.
3.	Use this table to understand how much friction is needed at each step of a userâ€™s journey

<div><img src="/public/images/cps/picture-1.jpg" alt="image of the risk levels" style="width:75%;height:auto"></div>

##  Step 3: Identify key moments
1.	Get familiar with the key steps in the â€˜Human oversight journeyâ€™ (see below)
2.	Pick out the steps that are relevant to your use case
3.	At each step, consider how you can use each type of friction to mitigate a risk or drive preferred outcomes

### Things to consider

ðŸ’¡Design for changing needs over time. For instance, first use/onboarding requires friction to help users understand the tool. 50th use does not.

ðŸ’¡ Design to avoid alarm fatigue.â€¨Only warn users, when meaningful, about specific concerns.


<div><img src="/public/images/cps/picture-2.jpg" alt="image of the human oversight journey" style="width:100%;height:auto"></div>

### Glossary

- AI outputs: In this context â€˜outputâ€™ means any material that an AI has generated, or a suggestion it has prescribed.
- User: The user is the person operating CPSâ€™ internal tool. Most likely a Prosecutor, Legal Manager or Casework Assistant

##  Step 4: Check for existing solutions
Itâ€™s likely that interaction patterns exist to help your team solve for some of these common needs. Check the Design Manual.

As and when you create something new, consider if it could be generalised and used repeatedly across other types of use case. If so, share it back to the Design Manual.




