Used when the user may want to check correctness of the outputs, or to see the justification for the output

<div><img src="/public/images/cps/showSources1.png" alt="Example show sources summary " style="width:100%;height:auto"></div>
<div><img src="/public/images/cps/showSources2.png" alt="Example from the legal and policy use case" style="width:100%;height:auto"></div>

## Where

During a task, once the AI outputs have been generated

## Mindful friction could help

- Spot errors with AI generated summaries
- Build confidence in the outputs when they’re correct

## Risks this pattern could mitigate

- utomation fails to handle factual content correctly
- Automation caution

## Don’ts

- Make it hard to verify the sources by making them available only by leaving the current task


## Needs testing

- Which types of AI outputs need to reference sources to be trustworthy, and which don’t
- When users may need to check the provided sources in more detail to check their veracity
- What’s the interaction pattern which allows reviewing sources in context without overloading the user