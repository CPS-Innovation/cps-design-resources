Used when there’s a need to demonstrate human-in-the-loop involvement and quality review done by a human

<div><img src="/public/images/cps/qualityAssure1.png" alt="Example quality assurance summary " style="width:100%;height:auto"></div>
<div><img src="/public/images/cps/qualityAssure1.png" alt="Example from the letter drafter use case" style="width:100%;height:auto"></div>

## Where

At the end of a journey, AI outputs are part of the work

## Mindful friction could help

- Set expectations about responsibilities of humans in the loop
- Provide transparency for the public, and a way to provide feedback about systems that affect them

## Risks this pattern could mitigate

- Unjust or unfair outcomes or processes
- Shifts in real and perceived responsibilities

## Don’ts

- Use more than once per task output
- Use when public accountability isn’t required


## Needs testing

- What’s the degree of disclosure regarding the use of AI that is transparent and useful to the public
- When to include the quality assurance proof and when it’s not relevant
- What information the public will find useful if they use the reference code, for example a simpler version of the 
 [algorithmic transparency record](https://www.gov.uk/algorithmic-transparency-records) 