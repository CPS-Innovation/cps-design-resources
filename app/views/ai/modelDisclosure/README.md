Used when the user may want to check the AI system responsible for the outputs

<div><img src="/public/images/cps/modelDisc1.png" alt="model disclosure example " style="width:100%;height:auto"></div>
<div><img src="/public/images/cps/modelDisc2.png" alt="Example from the legal and policy use case" style="width:100%;height:auto"></div>


## Where

Before the outputs are shown, or presented alongside the outputs

## Mindful friction could help

- Inform about key features of the model, includinggovernance and monitoring processes
- Establish that output may not always be right and provide a way to report issues

## Risks this pattern could mitigate

- Automation complacency
- Shifts in real and perceived responsibility
- Outputs become lower quality over time

## Donâ€™ts

- Overload the pattern with too much information upfront


## Needs testing

- Where in the journey are the appropriate points to include model disclosure
- How much information to include upfront, and how muchto progressively disclose on interaction
- Should there be a place where users can always find out model information for all AI features in the system